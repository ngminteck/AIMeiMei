{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29e7d2-7c65-48ec-9fe7-80a747300df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e09b263-9ed9-4ac2-814b-4dbbd659b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a97097-5e8e-40b5-8b45-62a1c0f284a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d0da7-d325-4ed4-b05c-4c442df77028",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "babddc74-c6e2-484e-b0f4-977d4cc232a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure the model at models folder\n",
    "#https://nusu-my.sharepoint.com/personal/e1330352_u_nus_edu/Documents/ISY5004%20Practice%20Module/Models/models/u2net.onnx\n",
    "#https://nusu-my.sharepoint.com/personal/e1330352_u_nus_edu/Documents/ISY5004%20Practice%20Module/Models/models/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e77bda-bc10-4f8a-9582-dca658bb00d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "U2-Net using CPU ExecutionProvider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 5168/5168 [2:07:13<00:00,  1.48s/it, 5168/5168 U2Net IoU: 0.7334, SAM IoU: 0.8967]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Overall Performance ---\n",
      "Average U2-Net IoU: 0.6151\n",
      "Average SAM IoU: 0.8544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnxruntime\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# ---------- Helper Functions ----------\n",
    "\n",
    "def preprocess_for_u2net(image, target_size=(320, 320)):\n",
    "    \"\"\"\n",
    "    Preprocess the image for U2-Net:\n",
    "      - Resize\n",
    "      - Convert BGR to RGB\n",
    "      - Normalize to [0, 1]\n",
    "      - Rearrange to CHW and add batch dimension\n",
    "    \"\"\"\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    image_resized = cv2.resize(image, target_size)\n",
    "    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n",
    "    image_norm = image_rgb.astype(np.float32) / 255.0\n",
    "    image_input = np.transpose(image_norm, (2, 0, 1))  # CHW\n",
    "    image_input = np.expand_dims(image_input, axis=0)  # add batch dim\n",
    "    return image_input, (orig_w, orig_h)\n",
    "\n",
    "def postprocess_u2net(prediction, orig_size, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Process U2-Net output:\n",
    "      - Remove batch and channel dimensions\n",
    "      - Resize to original image size\n",
    "      - Binarize using threshold\n",
    "    \"\"\"\n",
    "    pred_mask = prediction[0, 0, :, :]\n",
    "    pred_mask = cv2.resize(pred_mask, orig_size)\n",
    "    pred_mask_bin = (pred_mask > threshold).astype(np.uint8) * 255\n",
    "    return pred_mask_bin\n",
    "\n",
    "def compute_iou(mask_pred, mask_gt):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) for two binary masks.\n",
    "    Assumes both masks are binary with values 0 or 255.\n",
    "    \"\"\"\n",
    "    mask_pred_bool = mask_pred.astype(bool)\n",
    "    mask_gt_bool = mask_gt.astype(bool)\n",
    "    intersection = np.logical_and(mask_pred_bool, mask_gt_bool).sum()\n",
    "    union = np.logical_or(mask_pred_bool, mask_gt_bool).sum()\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def ensure_dir(dir_path):\n",
    "    \"\"\"Create directory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "def get_random_points_from_mask(mask, n_points=10):\n",
    "    \"\"\"\n",
    "    From the binary mask, randomly sample up to n_points coordinates\n",
    "    where the mask is foreground (non-zero).\n",
    "    Returns an array of shape [N, 2] where N <= n_points.\n",
    "    \"\"\"\n",
    "    # Get indices of foreground pixels\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    coords = np.stack([xs, ys], axis=1)\n",
    "    if len(coords) == 0:\n",
    "        # If no foreground, fallback to image center\n",
    "        h, w = mask.shape\n",
    "        return np.array([[w // 2, h // 2]])\n",
    "    # If more than n_points, randomly sample without replacement\n",
    "    if len(coords) > n_points:\n",
    "        indices = np.random.choice(len(coords), size=n_points, replace=False)\n",
    "        coords = coords[indices]\n",
    "    return coords\n",
    "\n",
    "def guided_sam_inference_with_multiple_points(predictor, image, prompt_points):\n",
    "    \"\"\"\n",
    "    Use SAM predictor with multiple prompt points.\n",
    "    For each prompt point, get a mask and then combine (union) all masks.\n",
    "    \"\"\"\n",
    "    predictor.set_image(image)\n",
    "    input_labels = np.ones(len(prompt_points), dtype=np.int32)  # all positive prompts\n",
    "    # SAM returns [N, H, W] if multimask_output=False and multiple points are given.\n",
    "    masks, _, _ = predictor.predict(\n",
    "        point_coords=prompt_points,\n",
    "        point_labels=input_labels,\n",
    "        multimask_output=False\n",
    "    )\n",
    "    # Combine all masks using a pixel-wise maximum (union)\n",
    "    combined_mask = np.zeros_like(masks[0], dtype=np.uint8)\n",
    "    for m in masks:\n",
    "        mask_uint8 = (m.astype(np.uint8)) * 255\n",
    "        combined_mask = cv2.bitwise_or(combined_mask, mask_uint8)\n",
    "    return combined_mask\n",
    "\n",
    "# ---------- Device Setup ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---------- Load Models ----------\n",
    "\n",
    "# 1. Load U2-Net (ONNX) with GPU support if available\n",
    "u2net_path = os.path.join(\"models\", \"u2net.onnx\")\n",
    "providers = onnxruntime.get_available_providers()\n",
    "if \"CUDAExecutionProvider\" in providers:\n",
    "    u2net_session = onnxruntime.InferenceSession(u2net_path, providers=[\"CUDAExecutionProvider\"])\n",
    "    print(\"U2-Net using CUDAExecutionProvider\")\n",
    "else:\n",
    "    u2net_session = onnxruntime.InferenceSession(u2net_path)\n",
    "    print(\"U2-Net using CPU ExecutionProvider\")\n",
    "\n",
    "# 2. Load SAM model using its registry (example: SamPredictor)\n",
    "sam_checkpoint = os.path.join(\"models\", \"sam_vit_h_4b8939.pth\")\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "# ---------- Setup Directories for Saving Results ----------\n",
    "\n",
    "image_dir = \"DUT-OMRON-image\"\n",
    "mask_dir = \"DUT-OMRON-mask\"\n",
    "\n",
    "# We'll save outputs using the naming convention:\n",
    "# {original file name}_mask.png, {original file name}_foreground.png, and {original file name}_background.png\n",
    "u2net_output_dir = \"u2net_result\"\n",
    "sam_output_dir = \"sam_result\"\n",
    "ensure_dir(u2net_output_dir)\n",
    "ensure_dir(sam_output_dir)\n",
    "\n",
    "csv_filename = \"compare_scores.csv\"\n",
    "\n",
    "# ---------- Processing and Evaluation ----------\n",
    "\n",
    "image_paths = sorted(glob(os.path.join(image_dir, \"*.*\")))\n",
    "iou_u2net_list = []\n",
    "iou_sam_list = []\n",
    "results = []\n",
    "\n",
    "pbar = tqdm(image_paths, total=len(image_paths), desc=\"Processing images\", leave=True)\n",
    "\n",
    "for i, image_path in enumerate(pbar):\n",
    "    filename = os.path.basename(image_path)\n",
    "    basename = os.path.splitext(filename)[0]\n",
    "    gt_mask_path = os.path.join(mask_dir, basename + \".png\")  # Adjust extension if needed\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    gt_mask = cv2.imread(gt_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None or gt_mask is None:\n",
    "        pbar.set_postfix_str(f\"Skipping {basename} (missing image or mask)\")\n",
    "        continue\n",
    "\n",
    "    # --- U2-Net Inference ---\n",
    "    input_tensor, orig_size = preprocess_for_u2net(image)\n",
    "    input_name = u2net_session.get_inputs()[0].name\n",
    "    pred = u2net_session.run(None, {input_name: input_tensor})[0]\n",
    "    u2net_mask = postprocess_u2net(pred, orig_size)\n",
    "    \n",
    "    # Generate filenames for U2-Net outputs\n",
    "    u2net_mask_filename = f\"{basename}_mask.png\"\n",
    "    u2net_fg_filename = f\"{basename}_foreground.png\"\n",
    "    u2net_bg_filename = f\"{basename}_background.png\"\n",
    "    \n",
    "    # Save U2-Net mask using the new naming convention\n",
    "    cv2.imwrite(os.path.join(u2net_output_dir, u2net_mask_filename), u2net_mask)\n",
    "    \n",
    "    # Create U2-Net foreground and background\n",
    "    foreground_u2net = cv2.bitwise_and(image, image, mask=u2net_mask)\n",
    "    background_u2net = cv2.bitwise_and(image, image, mask=cv2.bitwise_not(u2net_mask))\n",
    "    \n",
    "    # Save U2-Net foreground and background images\n",
    "    cv2.imwrite(os.path.join(u2net_output_dir, u2net_fg_filename), foreground_u2net)\n",
    "    cv2.imwrite(os.path.join(u2net_output_dir, u2net_bg_filename), background_u2net)\n",
    "\n",
    "    # --- SAM Inference ---\n",
    "    # Here, prompt points are derived from the ground truth mask (for demonstration)\n",
    "    prompt_points = get_random_points_from_mask(gt_mask, n_points=10)\n",
    "    sam_mask = guided_sam_inference_with_multiple_points(sam_predictor, image, prompt_points)\n",
    "    \n",
    "    # Generate filenames for SAM outputs\n",
    "    sam_mask_filename = f\"{basename}_mask.png\"\n",
    "    sam_fg_filename = f\"{basename}_foreground.png\"\n",
    "    sam_bg_filename = f\"{basename}_background.png\"\n",
    "    \n",
    "    # Save SAM mask using the new naming convention\n",
    "    cv2.imwrite(os.path.join(sam_output_dir, sam_mask_filename), sam_mask)\n",
    "    \n",
    "    # Create SAM foreground and background images\n",
    "    foreground_sam = cv2.bitwise_and(image, image, mask=sam_mask)\n",
    "    background_sam = cv2.bitwise_and(image, image, mask=cv2.bitwise_not(sam_mask))\n",
    "    \n",
    "    # Save SAM foreground and background images\n",
    "    cv2.imwrite(os.path.join(sam_output_dir, sam_fg_filename), foreground_sam)\n",
    "    cv2.imwrite(os.path.join(sam_output_dir, sam_bg_filename), background_sam)\n",
    "\n",
    "    # --- Evaluation (IoU) ---\n",
    "    # Evaluate both U2-Net and SAM predictions against the ground truth\n",
    "    gt_mask_bin = (gt_mask > 0).astype(np.uint8) * 255\n",
    "    iou_u2net = compute_iou(u2net_mask, gt_mask_bin)\n",
    "    iou_sam = compute_iou(sam_mask, gt_mask_bin)\n",
    "    iou_u2net_list.append(iou_u2net)\n",
    "    iou_sam_list.append(iou_sam)\n",
    "    results.append([filename, iou_u2net, iou_sam])\n",
    "\n",
    "    # Update the progress bar with the latest IoU scores\n",
    "    pbar.set_postfix_str(\n",
    "        f\"{i+1}/{len(image_paths)} U2Net IoU: {iou_u2net:.4f}, SAM IoU: {iou_sam:.4f}\"\n",
    "    )\n",
    "\n",
    "# After processing all images, write the CSV file.\n",
    "with open(csv_filename, mode='w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"Image\", \"U2-Net IoU\", \"SAM IoU\"])\n",
    "    writer.writerows(results)\n",
    "\n",
    "# ---------- Summary of Results ----------\n",
    "if iou_u2net_list and iou_sam_list:\n",
    "    avg_iou_u2net = np.mean(iou_u2net_list)\n",
    "    avg_iou_sam = np.mean(iou_sam_list)\n",
    "    print(\"\\n--- Overall Performance ---\")\n",
    "    print(f\"Average U2-Net IoU: {avg_iou_u2net:.4f}\")\n",
    "    print(f\"Average SAM IoU: {avg_iou_sam:.4f}\")\n",
    "else:\n",
    "    print(\"No valid results to summarize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa2585-0a33-47d6-88b2-988073f29c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
